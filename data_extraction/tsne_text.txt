BERT contextual sequence embeddings visualization:
Transformer models are preeminent due to their potential to incorporate context during learning, primarily attributable to self-attention coupled with high-dimensional vectors reflecting multiple degrees of information abstraction embedded within the model's layers. Moreover, high-dimensional vectors cannot be inspected or analyzed to validate the learning process and the type of captured information. However, utilizing nonlinear dimensionality reduction techniques, the dimensionality of the embeddings can be projected onto a lower-dimensional space, thereby reducing the embeddings to an interpretable human subspace (two or three-dimensional cartesian coordinate system). One prominent method is t-distributed stochastic neighbor embedding (t-SNE), which projects nearby points in a high-dimensional manifold closer together in a lower-dimensional space than non-neighboring points. t-SNE consists of two phases to achieve its objectives.
The first step includes assigning pairs of similar high-dimensional points with a higher probability than non-similar pairings. The second step is identical to the first only for lower-dimensional points, and then the Kullbackâ€“Leibler divergence between the two computed probability distributions is minimized to maintain the structure as much as possible with a low projection error rate. The resulting projections can vary depending on the starting parameters of t-SNE. By projecting the contextualized embeddings of each layer of a given transformer using t-SNE, the following component illustrates each layer's ability to uncover patterns and discrimination boundaries on a given dataset for a particular task along with the progression of training epochs. Additionally, the component is modular to cover all changeable components and inputs, such as the model, task, dataset, number of training epochs, various hyperparameters, and layers to inspect. 

The following example applies the tool to the sequence classification domain by fine-tuning the pre-trained Bert-base-uncased model on binary and multi-class text classification tasks taken from the TweetEval dataset (hate, offensive, and sentiment). Moreover, around 2000 training samples are taken from each dataset and then preprocessed initially by encoding noise-generating factors such as URLs, hashtags, usernames, and emojis, removing extra spaces and lowering the text. The preprocessed text is then tokenized by Bert's fast tokenizer provided by the hugging face framework and loaded in a batch sampler to feed the model throughout the training process dynamically. The model is trained for four epochs using a batch size of 16 and a learning rate of 1e-5, and leveraging the maximum sequence length of 512, shorter or longer input samples are padded or truncated, respectively. Furthermore, openTSNE is deployed and used for the dimensionality reduction phase due to its highly efficient and parallel implementation. Additionally, t-SNE is initialized with a perplexity of 500, which preserve the distance between each point and its 500 closest neighbors. A higher number of maintained neighbors allows covering the global structure reasonably. t-SNE also provides the possibility of applying PCA to the input samples as an initial reduction technique followed by the actual procedure. At the completion of each training epoch, the initialized t-SNE function is then applied to the averaged embedding from across all non-masked tokens in the sequence of each training sample along each model layer resulting in a (5, 12, 2000, 2) matrix containing all 2d projections for each layer across each training epoch. Each layer is then depicted in a different figure, and each data sample is mapped to a specific color corresponding to the respective class. 

The results of the following visualization hold two main observations:
1. The data points of different classes are highly mixed, and no pattern or discrimination boundaries are yet developed at the beginning of the training loop. As the training progresses, an apparent clustering of the different classes starts to establish itself in some layers.
2. The pattern and clustering of the different classes are primarily evident in higher layers of the model.

The previous observations show that the pre-trained Bert model has a low or non-existing understanding of unseen data, but after a proper fine-tuning procedure, it can generalize and adapt to new domains effectively. Furthermore, the observation shows which layers learn and hold the most discriminating features.

The code was adapted and customized from the following site:  
https://www.kaggle.com/code/tanmay17061/transformers-bert-hidden-embeddings-visualization/notebook

Technology stack:
* Hugging face 
* openTSNE


